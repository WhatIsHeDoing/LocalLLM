"""
This script reads the database of information from local text files
and uses a large language model to answer questions about their content.
"""

from halo import Halo
from time import monotonic
import warnings

# Hide LangChainDeprecationWarning...
with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    from langchain_community.document_loaders import DirectoryLoader, TextLoader
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.llms import CTransformers
    from langchain_community.vectorstores import FAISS
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    from langchain.text_splitter import RecursiveCharacterTextSplitter

binary_directory = "bin"
"""Directory of the local binary files."""

llm_spinner = Halo("Loading the Large Language Model...", spinner="dots")
llm_spinner.start()
llm_load_start_time = monotonic()

llm = CTransformers(
    model=f"./{binary_directory}/llama-2-7b-chat.ggmlv3.q2_K.bin",
    model_type="llama",
    config={"max_new_tokens": 256, "temperature": 0.01},
)

llm_spinner.stop_and_persist("üß†", "Large Language Model loaded")

print("üìÇ Loading the information interpreted from local files...")

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": "cpu"}
)

db = FAISS.load_local(binary_directory, embeddings)

print("üîÉ Preparing a version of the LLM preloaded with the local content...")
retriever = db.as_retriever(search_kwargs={"k": 2})

template = """
Use the following information to answer the question from the user.
Do not try to make up an answer if you do not know it.
Context: {context}
Question: {question}
# Only return the helpful answer below.
# Helpful answer:
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_llm = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)

print("‚åõ Prepared in", round(monotonic() - llm_load_start_time), "seconds")
print("üöÄ The AI chatbot is ready to answer your questions!")

while True:
    print()
    query = input("üó®Ô∏è Ask your next question or task, or leave blank to exit:\n")

    if not query:
        break

    query_spinner = Halo(text="Answering...", spinner="dots")
    query_spinner.start()
    query_start_time = monotonic()
    output = qa_llm({"query": query})

    query_spinner.stop_and_persist("üí¨", "The chatbot responded with:")
    print(output["result"])
    print("‚åõ Answered in", round(monotonic() - query_start_time), "seconds")

print("üëã Closing the app...")
